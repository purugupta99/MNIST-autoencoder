# -*- coding: utf-8 -*-
"""q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ljD1W3kBH7O7JkY5q_huNb6wsZG2eLAF

## Simple Model -- Undercomplete AE
"""

# Code here
import torch
import torch.nn as nn
import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
import torch.nn.init as weight_init
import matplotlib.pyplot as plt
import pdb
from torch.utils.data.sampler import SubsetRandomSampler
from sklearn.metrics import mean_squared_error
from sklearn.svm import SVC


#parameters
batch_size = 128

preprocess = transforms.Compose([
                           transforms.ToTensor(),
                           transforms.Normalize((0.1307,), (0.3081,))
                       ])

#Loading the train set file
dataset = datasets.MNIST(root='./data',
                            transform=preprocess,  
                            download=True)

loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)

"""### Autoencoder Class"""

class AE(nn.Module):
    def __init__(self, encode_sz):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(28*28, 256),
            nn.ReLU(),
            nn.Linear(256,64),
            nn.ReLU(),
            nn.Linear(64,encode_sz),
        )
        self.decoder = nn.Sequential(
            nn.Linear(encode_sz, 64),
            nn.ReLU(),
            nn.Linear(64, 256),
            nn.ReLU(),
            nn.Linear(256, 28*28),
            nn.Tanh()
        )
    
    def forward(self,x):
        h = self.encoder(x)
        xr = self.decoder(h)
        return xr,h

#Misc functions
def loss_plot(losses):
    max_epochs = len(losses)
    times = list(range(1, max_epochs+1))
    plt.figure(figsize=(30, 7))
    plt.xlabel("epochs")
    plt.ylabel("cross-entropy loss")
    return plt.plot(times, losses)

use_cuda = torch.cuda.is_available()
device = torch.device("cuda" if use_cuda else "cpu")
print('Using CUDA ', use_cuda)


#Optimizer and Scheduler
# optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)
# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, threshold=0.001, patience=5, verbose = True)

# Commented out IPython magic to ensure Python compatibility.
optimisers = ['adam', 'rmsprop', 'sgd_with', 'sgd_without']

hls = [2, 5, 7, 15, 20]

num_epochs = 5

#Training

loss_opt = []

for opt in optimisers:

    loss_hl = []

    for hl in hls:
        net = AE(hl)
        net = net.to(device)

        #Mean square loss function
        criterion = nn.MSELoss()

        #Parameters
        learning_rate = 1e-2
        weight_decay = 1e-5

        if opt == 'adam':
            optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)
        elif opt == 'rmsprop':
            optimizer = torch.optim.RMSprop(net.parameters(), lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)
        elif opt == 'sgd_with':
            optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate,momentum=0.9)
        elif opt == 'sgd_without':
            optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate,momentum=0)

        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, threshold=0.001, patience=5, verbose = True)
        epochLoss = []
        for epoch in range(num_epochs):
            total_loss, cntr = 0, 0
            
            for i,(images,_) in enumerate(loader):
                
                images = images.view(-1, 28*28)
                images = images.to(device)
                
                # Initialize gradients to 0
                optimizer.zero_grad()
                
                # Forward pass (this calls the "forward" function within Net)
                outputs, _ = net(images)
                
                # Find the loss
                loss = criterion(outputs, images)
                
                # Find the gradients of all weights using the loss
                loss.backward()
                
                # Update the weights using the optimizer and scheduler
                optimizer.step()
              
                total_loss += loss.item()
                cntr += 1
            
            scheduler.step(total_loss/cntr)
            print ('Epoch [%d/%d], Loss: %.4f' 
#                           %(epoch+1, num_epochs, total_loss/cntr))
            epochLoss.append(total_loss/cntr)
        loss_hl.append(epochLoss[num_epochs-1])
    loss_opt.append(loss_hl)

fig = plt.figure()
plt.figure(figsize=(20,10))

i = 0
for opt in optimisers:
    plt.subplot(2,2,i+1)
    plt.title("Optimiser = "+ str(opt))
    times = list(range(1, num_epochs+1))
    plt.xlabel("epochs")
    plt.ylabel("cross-entropy loss")
    plt.plot(times, loss_opt[i])
    i += 1

plt.show()